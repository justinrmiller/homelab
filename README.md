# üè° Homelab Services Dashboard

A unified Streamlit dashboard for interacting with your local **Valkey**, **Kafka**, **Qdrant**, **PostgreSQL**, and **MongoDB** services.  
Easily monitor, manage, and experiment with these core open-source infrastructure tools‚Äîall from your browser.

---

## ‚ú® Features

- **Service Health Overview**: See live connection status for all services.
- **Valkey**: Set/get keys, list all keys, view server info.
- **Kafka**: List/create topics, produce and consume messages.
- **Qdrant**: Manage collections, upload/search vectors.
- **PostgreSQL**: List/create databases and tables, insert/query data, run custom SQL.
- **MongoDB**: List collections, insert/query documents.
- **Hasura**: Hasura GraphQL engine to interact with PostgreSQL.
- **Open WebUI**: Open-source web UI for local LLMs (Ollama backend required).
- **Quick Links**: Access official documentation for each service.

---

## üöÄ Getting Started

### üß∞ Prerequisites

- [Docker](https://www.docker.com/products/docker-desktop)
- [Docker Compose](https://docs.docker.com/compose/)
- [Python 3.8+](https://www.python.org/downloads/) (for local Streamlit development)

### üì• Clone the Repository

```sh
git clone https://github.com/justinrmiller/homelab.git
cd homelab
```

---

## üèÉ‚Äç‚ôÇÔ∏è Running the Stack

### 1Ô∏è‚É£ Start All Services

```sh
docker compose up -d
```

This launches:

- Valkey
- Kafka
- Qdrant
- PostgreSQL
- MongoDB
- Hasura
- The Streamlit dashboard
- Open WebUI (for Ollama)

### 2Ô∏è‚É£ Install Python Dependencies (for local Streamlit development)

```sh
cd streamlit
pip install -r requirements.txt
```

### 3Ô∏è‚É£ Run the Streamlit Dashboard

You can run the dashboard inside Docker (recommended) or locally:

**Inside Docker**:  
The dashboard will be available at [http://localhost:8501](http://localhost:8501).

**Locally**:

```sh
cd streamlit
streamlit run app.py
```

---

## ‚öôÔ∏è Configuration

All service connection parameters are set via environment variables in `docker-compose.yml`.  
You can override them in your shell or with a `.env` file for local development.

**Example environment variables:**

- `VALKEY_HOST`, `VALKEY_PORT`
- `KAFKA_BOOTSTRAP_SERVERS`
- `QDRANT_HOST`, `QDRANT_PORT`
- `POSTGRES_HOST`, `POSTGRES_PORT`, `POSTGRES_USER`, `POSTGRES_PASSWORD`, `POSTGRES_DB`
- `MONGODB_HOST`, `MONGODB_PORT`, `MONGODB_USER`, `MONGODB_PASSWORD`, `MONGODB_DB`
- `HASURA_HOST`, `HASURA_PORT`

---

## üìÅ File Structure

```
homelab/
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ streamlit/
‚îÇ   ‚îú‚îÄ‚îÄ app.py
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ ... (other service configs)
```

---

## üñ•Ô∏è Usage

- Open [http://localhost:8501](http://localhost:8501) in your browser.
- Use the sidebar to select a service.
- Each service page provides interactive controls for common operations.

---

## üìö Service Documentation

- [Valkey Documentation](https://valkey.io/docs/)
- [Kafka Documentation](https://docs.confluent.io/platform/current/kafka/introduction.html)
- [Qdrant Documentation](https://qdrant.tech/documentation/)
- [PostgreSQL Documentation](https://www.postgresql.org/docs/17/index.html)
- [MongoDB Documentation](https://www.mongodb.com/docs/)
- [Hasura Documentation](https://hasura.io/docs/latest/)
- [Open WebUI Documentation](https://github.com/open-webui/open-webui)
- [Ollama Documentation](https://ollama.com/docs)

---

## üõ†Ô∏è Troubleshooting

- If a service is not connecting, check its logs:
  ```sh
  docker compose logs <service-name>
  ```
- Ensure ports are not in use by other processes.
- For local development, ensure your Python environment matches `requirements.txt`.

---

## üìÑ License

MIT License

---

## üë§ Author

- [justinrmiller](https://github.com/justinrmiller)
- [claude](https://www.anthropic.com/claude)

---

## ‚ÑπÔ∏è Notes

- **Open WebUI** is included for managing and chatting with local LLMs, but **Ollama** (the backend LLM server) must be installed separately on your machine. You can download and install Ollama from [https://ollama.com/](https://ollama.com/).
- All model downloads and management are handled via the Ollama CLI or web interface, not from this dashboard.
